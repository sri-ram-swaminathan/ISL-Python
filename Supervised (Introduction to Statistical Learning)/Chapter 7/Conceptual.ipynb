{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c2e18c-c4ce-4967-919b-b2887402980a",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "It was mentioned in this chapter that a cubic regression spline with one knot at $\\xi$ can be obtained using a basis of the form $x, x^{2}, x^{3},(x − \\xi)^{3}_{+}$, where $(x − \\xi)^{3}_{+} = (x − \\xi)^{3}$ if $x > \\xi$ and 0 otherwise.\n",
    "\n",
    "We will now show that a function of the form $f(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}(x − \\xi)^{3}_{+}$ is indeed a cubic regression spline, regardless of the values of $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}$\n",
    "\n",
    "(a) Find a cubic polynomial $f_{1}(x)=a_{1}+b_{1}x+c_{1}x^{2}+d_{1}x^{3}$ such that $f(x)=f_{1}(x)$ for all $x \\leq \\xi$. Express $a_{1},b_{1},c_{1},d_{1}$ in terms of $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}$.\n",
    ">- When $x \\leq \\xi$ then $(x − \\xi)^{3}_{+}=0$. Hence $f(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3}$\n",
    ">- On mapping the two functions we get that : $a_{1}=\\beta_{0},\\hspace{3mm} b_{1}=\\beta_{1}, \\hspace{3mm}c_{1}=\\beta_{2}, \\hspace{3mm}d_{1}=\\beta_{3}$\n",
    "\n",
    "(b) Find a cubic polynomial $f_{2}(x)=a_{2}+b_{2}x+c_{2}x^{2}+d_{2}x^{3}$ such that $f(x)=f_{2}(x)$ for all $x > \\xi$. Express $a_{2},b_{2},c_{2},d_{2}$ in terms of $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}$. We have now established that $f(x)$ is a piecewise polynomial.\n",
    ">- In this case, $f(x)=\\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}(x − \\xi)^{3}$\n",
    ">- $\\Rightarrow f(x)=\\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\beta_{4}(x^{3}-3x^{2}\\xi+3x \\xi^{2} -\\xi^{3})$\n",
    ">- $\\Rightarrow f(x)= \\beta_{0}-\\xi^{3}\\beta_{4} + (\\beta_{1}+3\\beta_{4}\\xi^{2})x + (\\beta_{2}-3\\beta_{4}\\xi)x^{2}+(\\beta_{3}+\\beta_{4})x^{3}$\n",
    ">- $a_{2}=\\beta_{0}-\\beta_{4}\\xi^{3}, \\hspace{3mm} b_{2}=\\beta_{1}+3\\beta_{4}\\xi^{2}, \\hspace{3mm} c_{2}=\\beta_{2}-3\\beta_{4}\\xi, \\hspace{3mm} d_{2}=\\beta_{3}+\\beta_{4}$\n",
    "\n",
    "(c) Show that $f_{1}(\\xi)=f_{2}(\\xi)$. That is, $f(x)$ is continuous at $\\xi$.\n",
    ">- $f_{1}(\\xi)= \\beta_{0} + \\beta_{1}\\xi + \\beta_{2}\\xi^{2} + \\beta_{3}\\xi^{3}$\n",
    ">- $f_{2}(\\xi)=\\beta_{0}-\\xi^{3}\\beta_{4} + (\\beta_{1}+3\\beta_{4}\\xi^{2})\\xi + (\\beta_{2}-3\\beta_{4}\\xi)\\xi^{2}+(\\beta_{3}+\\beta_{4})\\xi^{3}$\n",
    ">- $f_{2}(\\xi)=\\beta_{0}\\textcolor{Blue}{-\\xi^{3}\\beta_{4}} + \\beta_{1}\\xi+\\textcolor{Red}{3\\beta_{4}\\xi^{3}} + \\beta_{2}\\xi^{2}\\textcolor{Red}{-3\\beta_{4}\\xi^{3}}+\\beta_{3}\\xi^{3}+\\textcolor{Blue}{\\beta_{4}\\xi^{3}}$\n",
    ">- $f_{2}(\\xi)= \\beta_{0} + \\beta_{1}\\xi + \\beta_{2}\\xi^{2} + \\beta_{3}\\xi^{3}$\n",
    "\n",
    "(d) Show that $f^{'}_{1}(\\xi)=f^{'}_{2}(\\xi)$. That is, $f^{'}(x)$ is continuous at $\\xi$.\n",
    ">- $f^{'}_{1}(x)= \\beta_{1} + 2\\beta_{2}x + 3\\beta_{3}x^{2}$\n",
    ">- $f^{'}_{1}(\\xi)= \\beta_{1} + 2\\beta_{2}\\xi + 3\\beta_{3}\\xi^{2}$\n",
    ">- $f^{'}_{2}(x)= \\beta_{1}+3\\beta_{4}\\xi^{2} + 2(\\beta_{2}-3\\beta_{4}\\xi)x+3(\\beta_{3}+\\beta_{4})x^{2}$\n",
    ">- $f^{'}_{2}(\\xi)= \\beta_{1}+\\textcolor{Green}{3\\beta_{4}\\xi^{2}} + 2\\beta_{2}\\xi\\textcolor{Green}{-6\\beta_{4}\\xi^{2}}+3\\beta_{3}\\xi^{2}+\\textcolor{Green}{3\\beta_{4}\\xi^{2}}$\n",
    ">- $f^{'}_{2}(\\xi)= \\beta_{1} + 2\\beta_{2}\\xi + 3\\beta_{3}\\xi^{2}$\n",
    "\n",
    "(e) Show that $f^{\"}_{1}(\\xi)=f^{\"}_{2}(\\xi)$. That is, $f^{\"}(x)$ is continuous at $\\xi$. Therefore, $f(x)$ is indeed a cubic spline. \n",
    ">- $f^{\"}_{1}(x)= 2\\beta_{2} + 6\\beta_{3}x$\n",
    ">- $f^{\"}_{1}(\\xi)= 2\\beta_{2} + 6\\beta_{3}\\xi$\n",
    ">- $f^{\"}_{2}(x)= 2(\\beta_{2}-3\\beta_{4}\\xi)+6(\\beta_{3}+\\beta_{4})x$\n",
    ">- $f^{\"}_{2}(\\xi)= 2\\beta_{2}\\textcolor{Purple}{-6\\beta_{4}\\xi}+6\\beta_{3}\\xi+\\textcolor{Purple}{6\\beta_{4}\\xi}$\n",
    ">- $f^{\"}_{2}(\\xi)= 2\\beta_{2} + 6\\beta_{3}\\xi$\n",
    "\n",
    "#### Q3\n",
    "Suppose we fit a curve with basis functions $b_{1}(X) = X, b_{2}(X) =(X − 1)^{2}I(X \\geq 1)$. (Note that $I(X ≥ 1)=1 $ for $X ≥ 1$ and 0 otherwise.) We fit the linear regression model $Y = \\beta_{0} + \\beta_{1}b_{1}(X) + \\beta_{2}b_{2}(X) + \\epsilon$ and obtain coefficient estimates $\\hat{\\beta_{0}}$ = 1, $\\hat{\\beta_{1}}$ = 1, $\\hat{\\beta_{1}}$ = −2. Sketch the estimated curve between X = −2 and X = 2. Note the intercepts, slopes, and other relevant information.\n",
    ">- $\\hat{y}=1 + b_{1}(x) -2 b_{2}(x)$\n",
    ">- $\\hat{y}= \\begin{cases} \n",
    "      1+x, & -2 \\leq x < 1 \\\\\n",
    "       1+x -2(x-1)^{2}, & 1\\leq x\\leq 2 \\\\\n",
    "   \\end{cases}\n",
    "$\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"3.jpg\" width=\"700\"/>    \n",
    "</center>\n",
    "</div>\n",
    "\n",
    ">- The y-intercept of the curve is the point (0,1)\n",
    ">- In the region $-2 \\leq x < 1$ the curve is a straight line with slope 1.\n",
    ">- In the region $1\\leq x\\leq 2$ the curve is a parabole with slope 5-4x and critical point x=5/4.\n",
    "\n",
    "#### Q5\n",
    "Consider two curves, $\\hat{g}_{1}$ and  $\\hat{g}_{2}$, defined by\n",
    "\n",
    "$\\hat{g}_{1}= \\text{argmin} (\\sum_{i=1}^{n}(y_{i}-g(x_{i}))^{2} + \\lambda \\int [g^{3}(x)]^{2}dx)$\n",
    "\n",
    "$\\hat{g}_{2}= \\text{argmin} (\\sum_{i=1}^{n}(y_{i}-g(x_{i}))^{2} + \\lambda \\int [g^{4}(x)]^{2}dx)$\n",
    "\n",
    "where $g^{m}$ represents the mth derivative of g.\n",
    "\n",
    "(a) As $\\lambda \\to \\infty$, will $\\hat{g}_{1}$ or $\\hat{g}_{2}$ have the smaller training RSS?\n",
    ">- As we increase $\\lambda$ we increase the bias in the model, moving towards the linear regression setting.\n",
    ">- $\\hat{g}_{1}$ will minimize a function of the form $ax^{2}+bx+c$ whereas $\\hat{g}_{2}$ will minimize a function of the form $ax^{3}+bx^{2}+cx+d$.\n",
    ">- Hence $\\hat{g}_{2}$ is more flexible and has a smaller training RSS.\n",
    "\n",
    "(b) As $\\lambda \\to \\infty$, will $\\hat{g}_{1}$ or $\\hat{g}_{2}$ have the smaller test RSS?\n",
    "> If the true relationship f is quadratic we expect $\\hat{g}_{1}$ to have smaller test RSS. For cubic relationships we expect $\\hat{g}_{2}$ to have a smaller test RSS.  \n",
    "\n",
    "(c) For $\\lambda = 0$, will $\\hat{g}_{1}$ or $\\hat{g}_{2}$ have the smaller training and test RSS?\n",
    "> At $\\lambda = 0$ both the curves will be highly flexible and fit all of the points, making them indistinguishable. They will have 0 training RSS and large test RSS due to over-fitting (high variance)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

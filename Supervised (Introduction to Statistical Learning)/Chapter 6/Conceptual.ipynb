{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800ae901-7640-4d0e-8596-26ef7c8f2f9b",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:\n",
    "\n",
    "(a) Which of the three models with k predictors has the smallest training RSS?\n",
    "> Best subset model will have the smallest training RSS, because it is a brute force method that checks over all possible subsets.\n",
    "\n",
    "(b) Which of the three models with k predictors has the smallest test RSS?\n",
    "> Best subset model will have the smallest test RSS as well, because while estimating test RSS using cross-validation the brute force method again picks the lowest possible RSS. \n",
    "\n",
    "(c) True or False:\n",
    "\n",
    "i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)variable model identified by forward stepwise selection.\n",
    "> True. The k variable model will have k predictors and the k+1 variable model will contain all these k predictors and the next best predictor. \n",
    " \n",
    "ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+1) variable model identified by backward stepwise selection.\n",
    "> True. The k+1 variable model will have p-(k+1) predictors and the k variable model will exclude the worst predictor and have p-k predictors. \n",
    "\n",
    "iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+1) variable model identified by forward stepwise selection.\n",
    "> False. While both the methods are similar, the way they pick the predictors are different and it is not necessary for the predictors picked by one to be subset of the predictors picked by the other. \n",
    "\n",
    "iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1) variable model identified by backward stepwise selection.\n",
    "> False.\n",
    "\n",
    "v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k+1) variable model identified by best subset selection.\n",
    "> False. The best possible predictors for k variables need not be included in the next (previous) model with k+1 (k-1) predictors. [Example](https://hastie.su.domains/ISLRv2_website.pdf#page=239)  \n",
    "\n",
    "#### Q3\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$\\sum_{i=1}^{n}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}\\beta_{j}x_{ij})^{2} \\hspace{2mm} \\text{subject to} \\hspace{2mm} \\sum_{j=1}^{p}|\\beta_{j}| \\leq s$\n",
    "\n",
    "for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "(a) As we increase s from 0, the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "iv. Steadily decrease.\n",
    "\n",
    "v. Remain constant.\n",
    "> iv. Increasing s makes the model more flexible. Hence the training error will decrease monotonically.\n",
    "\n",
    "(b) Repeat (a) for test RSS.\n",
    ">ii. At first the decrease in bias reduces the test RSS but after a certain threshold the increase in variance will cause the test error to rise, leading to overfitting. \n",
    "\n",
    "(c) Repeat (a) for variance.\n",
    ">iii. As  the model  becomes more flexible, variance will increase monotonically. \n",
    "\n",
    "(d) Repeat (a) for (squared) bias.\n",
    ">iv. As  the model  becomes more flexible, the squared bias will decrease monotonically.\n",
    "\n",
    "(e) Repeat (a) for the irreducible error.\n",
    ">v. By definition the irreducible error is independent of the model's flexibility and represents the noise / random variation in the data. \n",
    "\n",
    "#### Q5 \n",
    "It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "Suppose that n = 2, p = 2, $x_{11}=x_{12}$ , $x_{21}=x_{22}$. Furthermore,  suppose that $y_{1}+y_{2}=0$ and $x_{11}+x_{21}=0$ and $x_{12}+x_{22}=0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{\\beta_{0}}=0$.\n",
    "\n",
    "(a) Write out the ridge regression optimization problem in this setting.\n",
    ">- $\\sum_{i=1}^{2}(y_{i}-\\sum_{j=1}^{2}\\beta_{j}x_{ij})^{2} + \\lambda \\sum_{j=1}^{2} \\beta_{j}^{2}$\n",
    ">- $J(\\beta_{1},\\beta_{2})=(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})^{2}+(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})^{2}+\\lambda \\beta_{1}^{2}+\\lambda \\beta_{2}^{2}$\n",
    "\n",
    "(b) Argue that in this setting, the ridge coefficient estimates satisfy $\\hat{\\beta_{1}} = \\hat{\\beta_{2}}$ .\n",
    ">- $\\frac{\\partial J}{\\partial \\beta_{1}}= 2(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})(-x_{11})+2(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})(-x_{21}) + 2\\lambda \\beta_{1}=0$\n",
    ">- $\\frac{\\partial J}{\\partial \\beta_{2}}= 2(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})(-x_{12})+2(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})(-x_{22}) + 2\\lambda \\beta_{2}=0$\n",
    ">- Recognizing that $x_{11}=x_{12}, x_{21}=x_{22}$ we substitute this in the second equation. Subtracting the two then yields $\\lambda(\\beta_{2}-\\beta_{1})=0\\Rightarrow \\beta_{1}=\\beta_{2}$\n",
    "\n",
    "(c) Write out the lasso optimization problem in this setting.\n",
    ">- $\\sum_{i=1}^{2}(y_{i}-\\sum_{j=1}^{2}\\beta_{j}x_{ij})^{2} + \\lambda \\sum_{j=1}^{2} |\\beta_{j}|$\n",
    ">- $J(\\beta_{1},\\beta_{2})=(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})^{2}+(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})^{2}+\\lambda |\\beta_{1}|+\\lambda |\\beta_{2}|$\n",
    "\n",
    "(d) Argue that in this setting, the lasso coefficients $\\hat{\\beta_{1}}$ and $\\hat{\\beta_{2}}$ are not unique in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.\n",
    ">- $\\frac{\\partial J}{\\partial \\beta_{1}}= 2(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})(-x_{11})+2(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})(-x_{21}) + \\lambda=0$\n",
    ">- $\\frac{\\partial J}{\\partial \\beta_{2}}= 2(y_{1}-\\beta_{1}x_{11}-\\beta_{2}x_{12})(-x_{12})+2(y_{2}-\\beta_{1}x_{21}-\\beta_{2}x_{22})(-x_{22}) + \\lambda =0$\n",
    ">- Applying the same trick in this case results in 0=0. That means that $\\beta_{1},\\beta_{2}$ are free variables and the problem is optimized for any value resulting in an $\\infty$ of solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

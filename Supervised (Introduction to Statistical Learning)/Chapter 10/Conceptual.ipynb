{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szoXRRdOi1wf"
      },
      "source": [
        "#### Q1\n",
        "1. Consider a neural network with two hidden layers: p = 4 input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output.\n",
        "\n",
        "(a) Draw a picture of the network, similar to Figures 10.1 or 10.4.\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"1.png\" width=\"500\"/>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "(b) Write out an expression for f (X), assuming ReLU activation functions. Be as explicit as you can!\n",
        "\n",
        ">- $f(X)= \\beta_{0}+\\sum_{l=1}^{K_{2}} \\beta_{l} A_{l}^{2}$\n",
        ">- $A_{l}^{2}= h_{l}^{2}(X)=g( w_{l0}^{2}+\\sum_{k=1}^{K_{1}}w_{lk}^{2}A_{k}^{1})$\n",
        ">- $A_{k}^{1}= h_{k}^{1}(X)=g( w_{k0}^{1}+\\sum_{j=1}^{p}w_{kj}^{1}X_{j})$\n",
        ">- Here, $g(x) = \\begin{array}{ c l }\n",
        "    0 & \\quad \\textrm{if } x < 0 \\\\\n",
        "    x                 & \\quad \\textrm{if } x \\geq 0\n",
        "  \\end{array}$\n",
        ">- $K_{2}=3, K_{1}=2$ and $p=4$\n",
        "\n",
        "(c) Now plug in some values for the coefficients and write out the value of f (X).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1b_zJ5i1wl",
        "outputId": "63a42bda-558b-49e9-a2fe-2c4e0bff0f02",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output of the neural network is: 0.6000000238418579\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden1 = nn.Linear(4, 2)  # First hidden layer (4 inputs -> 2 units)\n",
        "        self.hidden2 = nn.Linear(2, 3)  # Second hidden layer (2 -> 3 units)\n",
        "        self.output = nn.Linear(3, 1)   # Output layer (3 -> 1 unit)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        x = torch.relu(self.hidden1(x))  # ReLU activation after first hidden layer\n",
        "        x = torch.relu(self.hidden2(x))  # ReLU activation after second hidden layer\n",
        "        x = self.output(x)               # No activation at the output layer\n",
        "        return x\n",
        "\n",
        "# Create a model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Manually set the weights and biases\n",
        "with torch.no_grad():\n",
        "    # Layer 1\n",
        "    model.hidden1.weight = nn.Parameter(torch.tensor([[0.2, -0.5, -0.4, 0.6],\n",
        "                                                      [0.3, 0.8, 0.1, -0.3]]))\n",
        "    model.hidden1.bias = nn.Parameter(torch.tensor([0.1, -0.2]))\n",
        "\n",
        "    # Layer 2\n",
        "    model.hidden2.weight = nn.Parameter(torch.tensor([[0.4, -0.6],\n",
        "                                                      [-0.1, 0.5],\n",
        "                                                      [0.2, -0.3]]))\n",
        "    model.hidden2.bias = nn.Parameter(torch.tensor([0.3, -0.4, 0.2]))\n",
        "\n",
        "    # Output layer\n",
        "    model.output.weight = nn.Parameter(torch.tensor([[0.7, -0.2, 0.5]]))\n",
        "    model.output.bias = nn.Parameter(torch.tensor([0.1]))\n",
        "\n",
        "# Fit on sample data\n",
        "\n",
        "X = torch.tensor([[0.5, -0.2, 0.1, 0.4]], dtype=torch.float32)\n",
        "output = model(X)\n",
        "print(\"The output of the neural network is:\", output.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtbtDtUOi1wp"
      },
      "source": [
        "(d) How many parameters are there?\n",
        "\n",
        "> (4+1)2 + (2+1)3 + (3+1)1 = 23\n",
        "\n",
        "#### Q3\n",
        "Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there\n",
        "are M = 2 classes.\n",
        "\n",
        ">- Equation 10.14 is $-\\sum_{i=1}^{n}\\sum_{m=0}^{9} y_{im} log(f_{m}(x_{i}))$ and equation 4.5 is $l(\\beta_{0},\\beta_{1})=\\prod_{i:y_{i}=1}p(x_{i})\\prod_{i^{'}:y_{i^{'}}=0}(1-p(x_{i}))$\n",
        ">- Hence $log(l)=\\sum_{i:y_{i}=1}log(p(x_{i})) +\\sum_{i^{'}:y_{i^{'}}=0}log(1-p(x_{i}))$\n",
        ">- Let's define indicator variables $y_{i0}, y_{i1}$ such that $y_{i0}=1 when x_{i}=1$ and $y_{i1}=1 when x_{i}=0$\n",
        ">- Then $log(l)=\\sum_{i}y_{i0}log(p(x_{i})) +\\sum_{i}y_{i1}log(1-p(x_{i}))$\n",
        ">- Since $p(x_{i})$ represent the output of logistic regression, they can be re-written as the final output f(x)\n",
        ">- $\\Rightarrow -log(l)=-(\\sum_{i}y_{i0}log(f_{0}(x_{i})) +\\sum_{i}y_{i1}log(f_{1}(x_{i})))=-\\sum_{i} \\sum_{m=0}^{1} y_{im} log(f_{m}(x_{i}))$\n",
        "\n",
        "#### Q5\n",
        "In Table 10.2 on page 426, we see that the ordering of the three methods with respect to mean absolute error is different from the ordering with respect to test set $R^{2}$. How can this be?\n",
        "\n",
        ">- According to MAE the ordering is Lasso > Linear Regression > Neural Network. And according to $R^{2}$ : Linear Regression > Neural Network > Lasso.\n",
        ">- $MAE = \\frac{1}{n} \\sum_{i=1}^{n}|y_{i}-\\hat{y_{i}}|$ and $R^{2}=\\frac{(y_{i}- y_{mean})^{2} -(y_{i}-\\hat{y_{i}})^{2} }{(y_{i}- y_{mean})^{2}}$\n",
        ">- MAE measures absolute maginutde of errors, on average. $R^{2}$ lies between 0 and 1 and measures how much of the variance in the data is explained by the model. One reason for the different ordering is that each metric is trying to capture a different relation.\n",
        ">- The lasso model with least parameters results in least MAE, which means it is not overfitting on training data. However, the high bias in the lasso prevents it from capturing the variability in the data.\n",
        ">- It's important to note that the differences in MAE and $R^{2}$ are very small for these methods and either of these methods can be used without a signifcant difference in performance.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

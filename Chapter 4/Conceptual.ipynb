{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a94b50-b617-440c-ac23-f9c6af352a56",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n",
    "> - $P(X)=\\frac{e^{\\beta_{0}+\\beta_{1}\\cdot X}}{1+e^{\\beta_{0}+\\beta_{1}\\cdot X}}$\n",
    "> - Hence $1-P(X)=\\frac{1}{1+e^{\\beta_{0}+\\beta_{1}\\cdot X}}$\n",
    "> - And therefore $\\frac{P(X)}{1-P(X)}=e^{\\beta_{0}+\\beta_{1}\\cdot X}$\n",
    "\n",
    "#### Q3\n",
    "This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, $X ∼ N (\\mu_{k}, \\sigma^{2}_{k})$. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic. \n",
    "\n",
    "*Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that $\\sigma^{2}_{1}=...=\\sigma^{2}_{k}$*\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"3.jpg\" width=\"700\"/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "#### Q5\n",
    "We now examine the differences between LDA and QDA.\n",
    "\n",
    "(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    ">- Training set : QDA  will perform better since it is more flexible and has lesser bias than LDA.\n",
    ">- Test set : LDA will perform better since the true decision boundary is linear, the increase in bias is not offset by the reduction in variance for QDA.\n",
    "\n",
    "(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    ">- Training set : QDA  will perform better since it is more flexible and has lesser bias than LDA.\n",
    ">- Test set : QDA will perform better since the true decision boundary is non-linear, the decrease in variance is not offset by the increase in bias for LDA.\n",
    "\n",
    "(c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n",
    ">- Linear decision boundary : The testing accuracy of QDA relative to LDA will decrease. QDA will tend to over-fit and add noise as a feature of the model.\n",
    ">- Non-linear decision boundary : The testing accuracy of QDA relative to LDA will increase. A flexible method improves with an increase in sample size, as opposed to a less flexible method. A larger n also helps prevent over-fitting. \n",
    "\n",
    "(d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n",
    "> False.\n",
    "\n",
    "#### Q7\n",
    "Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on X, last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was $\\bar{X}=10$, while the mean for those that didn’t was $\\bar{X}=0$. In addition, the variance of X for these two sets of companies was $\\sigma^{2}$ = 36. Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.\n",
    "\n",
    "*Hint: Recall that the density function for a normal random variable is $f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma^{2}}e^{-\\frac{(x-\\mu_{k})^{2}}{2\\sigma^{2}}}$. You will need to use Bayes’ theorem.*\n",
    ">- We need to find P(Y=Yes|X=4). Using Bayes' theorem $P=\\frac{\\pi_{K} \\cdot f_{k}(x)}{\\sum_{l=1}^{K} \\pi_l \\cdot f_{l}(x)}$\n",
    ">- Given : (a) $\\pi_{Yes}=0.8$ (b) $\\pi_{No}=0.2$(c) $\\mu_{Yes}=10$ (d) $\\mu_{No}=0$ (e) $\\sigma^{2}=36$\n",
    ">- $f_{Yes}(4)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(4-10)^{2}}{2 \\cdot 36}}=0.241$\n",
    ">- $f_{No}(4)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(4-0)^{2}}{2 \\cdot 36}}=0.319$\n",
    ">- $\\Rightarrow$ P(Y=Yes|X=4)=$\\frac{\\pi_{Yes} \\cdot f_{Yes}(4)}{\\pi_{Yes} \\cdot f_{Yes}(4)+ \\pi_{No} \\cdot  f_{No}(4)}=\\frac{0.8 * 0.241}{0.8 * 0.241 + 0.2 * 0.319}=0.751$\n",
    ">- Since P > 0.5, we predict that the company will issue a dividend this year.  \n",
    "\n",
    "#### Q9\n",
    "This problem has to do with odds.\n",
    "\n",
    "(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n",
    ">- $Odds=\\frac{p}{1-p} \\Rightarrow  0.37=\\frac{p}{1-p}$\n",
    ">- $p + 0.37p = 0.37 \\Rightarrow p = \\frac{0.37}{1+0.37}$\n",
    ">- $p = 0.27$, which means that on average 1 in 4 (3.7) people will default.\n",
    "\n",
    "(b) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?\n",
    "> $Odds=\\frac{p}{1-p}=\\frac{0.16}{0.84}=0.19$\n",
    "\n",
    "#### Q12\n",
    "<div>\n",
    "<img src=\"12.jpg\" width=\"500\"/>  \n",
    "</div>\n",
    "\n",
    ">(a) Log odds are given by $ln(\\frac{p}{1-p})$. For logistic regression the log odds are $\\beta_{0}+\\beta_{1}\\cdot x$\n",
    ">\n",
    ">(b) For softmax regression, the log odds are $\\alpha_{orange0}-\\alpha_{apple0}+(\\alpha_{orange1}-\\alpha_{apple1}) \\cdot x$\n",
    ">\n",
    ">(c) We know that when fitting on the same data, the log odds remain the same regardless of the coding. Hence we can compare the coefficients in (a) and (b). This gives us two linear equations / constraints : (i) $\\alpha_{orange0}-\\alpha_{apple0}=2$ and (ii) $\\alpha_{orange1}-\\alpha_{apple1}=-1$\n",
    ">\n",
    ">(d) Similarly, this gives us $\\beta_{0}=-1.8$ and $\\beta_{1}=-2.6$\n",
    ">\n",
    ">(e) We need to compare the decision boundaries for both the models, evaluated with the Bayes' rule i.e. p=0.5. Now in part (d) we have manufactured the coefficients in the linear model to be the same, hence we expect that they will have the same decision boundary. We can check this by solving for x using the log odds : (i) logistic regression $-1.8-2.6 \\cdot  x =0 \\Rightarrow x = -0.69$ (ii) softmax regression $1.2-3 + (-2-0.6)\\cdot x =0 \\Rightarrow x =-0.69.$ Our class labels match all the time :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
